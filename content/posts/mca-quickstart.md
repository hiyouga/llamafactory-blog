---
date: '2025-10-21T16:21:12+08:00'
draft: true
title: 'Megatron Full Finetune with LLaMaFactory'
author: 'LLaMaFactory Team'
---
# LLaMaFactoryâœ–ï¸Mcore Adapter
ä¸ºäº†åˆ©ç”¨ä¸Šmegatron-coreä¸­çš„å„é¡¹å¹¶è¡ŒæŠ€æœ¯å’ŒGroupGEMMï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆ[ROLLå›¢é˜Ÿ](https://github.com/alibaba/ROLL)æä¾›çš„mcore_adapterï¼Œç»“åˆllamafactoryçš„æ•°æ®é“¾è·¯å’Œmegatron-trainerçš„è®­ç»ƒåç«¯ï¼Œæä¾›ä¸€ä¸ªæ–°çš„æ¨¡å‹è®­ç»ƒå·¥ä½œæµã€‚


## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ğŸ’» ç¯å¢ƒå®‰è£…
> ğŸ“¦ pip
```bash
# for megatron-core
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124

pip install \
    numpy==1.26.4 \
    optree>=0.13.0 \
    spacy==3.7.5 \
    weasel==0.4.1 \
    transformer-engine[pytorch]==2.2.0 \
    megatron-core==0.13.0 \
    deepspeed==0.16.4 

pip uninstall -y opencv opencv-python opencv-python-headless
pip install opencv-python-headless==4.11.0.86
pip install "git+https://github.com/alibaba/roll.git#subdirectory=mcore_adapter"

# for llamafactory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
```
> ğŸ³ docker(æ¨è)

å‚è€ƒ[dockerfile](https://github.com/Kuangdd01/LLaMA-Factory-X/blob/1cef3e5f3d06146442c60bedbb88af529f174512/docker/docker-cuda/Dockerfile.megatron)è¿›è¡Œæ„å»º

### 2. ğŸ¯ å¯åŠ¨å®éªŒ
> ğŸ–¥ï¸ å•æœºå…«å¡(80gb)
```bash
cd LLaMA-Factory
# qwen2_vl_full
USE_MCA=1 llamafactory-cli train examples/megatron/qwen2_vl_full.yaml
# qwen3_moe_full
USE_MCA=1 llamafactory-cli train examples/megatron/qwen3_moe_full.yaml
```
> ğŸŒ å¤šæœºå®éªŒ
```bash
export DISTRIBUTED_ARGS="
    --nproc_per_node 8 \
    --nnodes $WORLD_SIZE \
    --node_rank $RANK \
    --master_addr $MASTER_ADDR \
    --master_port $MASTER_PORT
"
USE_MCA=1 torchrun $DISTRIBUTED_ARGS src/train.py \
    --model_name_or_path ../model/qwen3_30b_a3b \
    --do_train \
    --stage sft \
    --finetuning_type full \
    --dataset identity \
    --preprocessing_num_workers 16 \
    --cutoff_len 4096 \
    --template qwen3_nothink \
    --output_dir saves/mca/qwen3_moe_full_id \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --max_steps 100 \
    --learning_rate 3e-6 \
    --logging_steps 1 \
    --save_steps 50 \
    --lr_scheduler_type constant \
    --bf16 \
    --tensor_model_parallel_size 1 \
    --sequence_parallel false \
    --pipeline_model_parallel_size 2 \
    --bias_activation_fusion true \
    --apply_rope_fusion true \
    --use_distributed_optimizer true \
    --overlap_param_gather true \
    --overlap_grad_reduce true \
    --moe_grouped_gemm true \
    --moe_token_dispatcher_type alltoall \
    --expert_model_parallel_size 4 \
    --recompute_granularity full
```

### 2.1 ğŸ”„ æƒé‡è½¬æ¢(mcore2hf)
```bash
python scripts/megatron_merge.py \
    --checkpoint_path saves/mca/qwen3_moe_full_id/checkpoint-50/ \
    --output_path saves/qwen3_moe_hf \
    --bf16
```

### 3. ğŸ’¡ Tips & æ³¨æ„äº‹é¡¹

#### 3.1 ğŸ“ Global Batch Size è®¡ç®—å·®å¼‚
åœ¨ä½¿ç”¨ Megatron è®­ç»ƒæ—¶ï¼Œæ³¨æ„ global batch size çš„è®¡ç®—ç›¸è¾ƒäºä¹‹å‰çš„è®¾ç½®æœ‰ç»†å¾®åŒºåˆ«ï¼š

**ğŸ“Œ å‚æ•°è¯´æ˜ï¼š**
- `ga`: gradient_accumulation_steps (æ¢¯åº¦ç´¯ç§¯æ­¥æ•°)
- `ws`: WORLD_SIZE (æ€»è¿›ç¨‹æ•°)
- `pp`: pipeline_model_parallel_size (æµæ°´çº¿å¹¶è¡Œå¤§å°)
- `tp`: tensor_model_parallel_size (å¼ é‡å¹¶è¡Œå¤§å°)
- `ep`: expert_model_parallel_size (ä¸“å®¶å¹¶è¡Œå¤§å°)

**ğŸ”¢ è®¡ç®—å…¬å¼å¯¹æ¯”ï¼š**
```bash
# åŸå§‹è®¡ç®—æ–¹å¼
origin_global_batch_size = ws * batchsize_per_device * ga

# MCA è®¡ç®—æ–¹å¼
mca_global_batch_size = (ws // pp // tp // ep) * batchsize_per_device * ga 
```

#### 3.2 âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®
- **ğŸ’¾ å†…å­˜ä¼˜åŒ–**: å¯ç”¨ `--use_distributed_optimizer` å’Œ `--overlap_param_gather` å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨
- **ğŸ“¡ é€šä¿¡ä¼˜åŒ–**: ä½¿ç”¨ `--overlap_grad_reduce` å¯ä»¥é‡å æ¢¯åº¦é€šä¿¡å’Œè®¡ç®—
- **ğŸ”§ MOE ä¼˜åŒ–**: å¯¹äº MOE æ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨ `--moe_token_dispatcher_type alltoall` å’Œ `--moe_grouped_gemm true` è·å¾—æ›´å¥½çš„æ€§èƒ½
- **âš™ï¸ å¹¶è¡Œä¼˜åŒ–**: `gradient_accumulation_steps` ä¸º PP çš„æ•´æ•°å€

#### 3.3 ğŸ” å¸¸è§é—®é¢˜æ’æŸ¥
1. **ğŸ’¥ OOM é”™è¯¯**: å‡å°‘ `per_device_train_batch_size` æˆ–å¢åŠ  `gradient_accumulation_steps`
2. **ğŸŒ é€šä¿¡è¶…æ—¶**: æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ `master_addr`ã€`master_port` è®¾ç½®
3. **âš™ï¸ å¹¶è¡Œåº¦è®¾ç½®**: ç¡®ä¿ `pp * tp * ep` èƒ½æ•´é™¤ `ws`
